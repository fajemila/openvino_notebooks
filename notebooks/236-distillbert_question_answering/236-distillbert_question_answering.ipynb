{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b649f56-f953-4583-9ea5-567320227cd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "The goal of this notebook is to show how to demonstrate inference of the DistilBERT model on OpenVINO ,DistilBERT is a popular pre-trained transformer-based model for natural language processing, the `distilbert-base-cased-distilled-squad` is trained to answer questions. We will first convert the pytorch model to ONNX then convert the ONNX model to an intermediate representation for optimization and deployment on a CPU using OpenVINO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89729d03-f2b0-4939-adfd-00ed2943f084",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "To follow this tutorial, you need to have Python 3.6 or later installed, along with the following libraries:\n",
    "\n",
    "- `numpy`\n",
    "- `openvino`\n",
    "- `torch`\n",
    "- `transformers`\n",
    "\n",
    "You can install these libraries using pip\n",
    "    <p>`pip install numpy onnxruntime openvino torch transformers`</p>\n",
    "However once you have installed the `requirements.txt` file you dont need to run the above line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f13f94-aa47-4418-a1b8-35f8ee4040a7",
   "metadata": {},
   "source": [
    "## Step 1: Load the Model and Tokenizer\n",
    "First, we need to load the DistilBERT model and tokenizer from the Hugging Face transformers library. The DistilBERT model is pre-trained on a large corpus of text and can be used for various NLP tasks, including question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37c7e7d-4c29-4042-9fe9-f28b88bd0e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 12:16:34.436328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 12:16:34.436387: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import openvino.runtime as ov\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "model_ckpt = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187af2b2-74fe-489a-b397-3a572f64a00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = Path(\"model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "MODEL_DIR = \"model/\"\n",
    "MODEL_DIR = f\"{MODEL_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27208bbb-e50d-4ab1-92c5-8e7046087713",
   "metadata": {},
   "source": [
    "## Step 2: Define the Question and Context\n",
    "Next, we need to define the question and context for which we want to find the answer. The question should be a string that represents the question, and the context should be a string that represents the text where the answer can be found. Since the tasks is for Extractive question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd2e855-8fe0-4320-8f05-0340a1dc72f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the question and context\n",
    "question = \"What is ONNX?\"\n",
    "context = \"ONNX (Open Neural Network Exchange) is an open standard format for representing machine learning models.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4df76-eea4-4bc8-a860-5f3d6540c9fb",
   "metadata": {},
   "source": [
    "## Step 3: Tokenize the Input\n",
    "Before we can pass the input to the model, we need to tokenize it using the DistilBERT tokenizer. The tokenizer converts the input into a sequence of tokens that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543e9b99-fb57-4476-995b-fba7747d3a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1c805-07c9-4511-88c2-1ad736ba8246",
   "metadata": {},
   "source": [
    "## Step 4: Perform Question Answering\n",
    "Now, we can pass the tokenized input to the DistilBERT model to perform question answering. The model returns two sets of logits: one for the start index of the answer span and one for the end index of the answer span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5378fd-2a4c-43b0-beea-692e018731e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the question answering task\n",
    "outputs = model(**inputs)\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21219ae0-c5bf-43cf-b7b0-82d66f5b785a",
   "metadata": {},
   "source": [
    "## Step 5: Find the Answer Span\n",
    "We need to find the start and end indices of the answer span in the context. We do this by finding the indices with the highest scores in the start and end logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288db967-2312-44c0-a40d-0b18290576b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the start and end indices of the answer span\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed67021-22c5-425e-894a-ae7bb6bc8444",
   "metadata": {},
   "source": [
    "## Step 6: Get the Answer Tokens and Decode Them\n",
    "Finally, we can retrieve the answer tokens from the input and decode them using the DistilBERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0156aff2-131c-4bb1-8787-7dc4a1638502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the answer tokens and decode them\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_index:end_index]\n",
    "answer = tokenizer.decode(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ffa46cd-047b-4511-a26b-0b347176af9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Neural Network Exchange\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d3767-ff36-418b-9606-48f112c5c891",
   "metadata": {},
   "source": [
    "## Step 7: Convert the Model to ONNX Format\n",
    "\n",
    "We can convert the PyTorch model to the ONNX format using the `torch.onnx.export` function. This function takes as inputs the PyTorch model, the input tensor, the output ONNX model name, and the names of the input and output tensors. We also need to specify the dynamic axes of the input and output tensors. Dynamic axes allow the input and output tensors to have a variable batch size and sequence length. Meaning we can enter length of various sizes as long as it is less than maximum input sizes for DistillBERT, and also process multiple inputs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507442d8-2d19-4428-8929-b212a89a339e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/codespaces-blank/openvino_env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:218: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ONNX format.\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output names for the ONNX model\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"start_logits\", \"end_logits\"]\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "onnx_path = Path(\"model/distillbert_qa.onnx\")\n",
    "\n",
    "# Export the model to the ONNX format\n",
    "torch.onnx.export(\n",
    "    model,  # model to export\n",
    "    (input_ids, attention_mask),  # input as tuple\n",
    "    onnx_path,  # output ONNX model name\n",
    "    input_names=input_names,  # names for input tensor\n",
    "    output_names=output_names,  # names for output tensor\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"start_logits\": {0: \"batch_size\"},\n",
    "        \"end_logits\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "print(\"Model saved in ONNX format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba7bed-900d-479e-a357-6188f971a2e6",
   "metadata": {},
   "source": [
    "## Step 8: Convert the Model to OpenVINO Format\n",
    "\n",
    "Now that we have our model in the ONNX format, we can use the OpenVINO toolkit to optimize the model for deployment on Intel hardware. We first need to use the OpenVINO Model Optimizer to convert the ONNX model to the OpenVINO Intermediate Representation (IR) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a12c2d4-5199-40bc-84fb-7e39c441c55d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /workspaces/codespaces-blank/openvino_notebooks/notebooks/236-distillbert_question_answering/model/distilbert-base-cased-distilled-squad.xml\n",
      "[ SUCCESS ] BIN file: /workspaces/codespaces-blank/openvino_notebooks/notebooks/236-distillbert_question_answering/model/distilbert-base-cased-distilled-squad.bin\n"
     ]
    }
   ],
   "source": [
    "# Construct the command for Model Optimizer.\n",
    "optimizer_command = f'mo \\\n",
    "                     --input_model {onnx_path} \\\n",
    "                     --output_dir {MODEL_DIR} \\\n",
    "                     --model_name {model_ckpt} \\\n",
    "                     --input input_ids,attention_mask \\\n",
    "                     --input_shape \"[1,512],[1,512]\"'\n",
    "\n",
    "! $optimizer_command\n",
    "\n",
    "core = ov.Core()\n",
    "ir_model_xml = str((Path(MODEL_DIR) / model_ckpt).with_suffix(\".xml\"))\n",
    "compiled_model = core.compile_model(ir_model_xml)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6da9d-fc8b-4c3a-a7cd-6ecb629d6970",
   "metadata": {},
   "source": [
    "## Step 9: Create a function to answer questions\n",
    "Now that we have our ONNX model and our tokenizer, we can create a function that takes a question and a context as inputs and returns an answer.\n",
    "\n",
    "First, we need to tokenize the input using the tokenizer. We will use the padding and truncation options to ensure that the input is of a fixed length (512 in this case) and that any extra text is truncated or padded with zeros as necessary.\n",
    "\n",
    "Once we have the input tokenized, we can use the infer method of the compiled OpenVINO model to get the start and end indices of the answer span. We will then decode the answer tokens using the tokenizer and return the resulting answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9132ef-86ab-4706-bcfb-06e1b71c0299",
   "metadata": {},
   "source": [
    "Now we can use this function to answer questions based on the input context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89bf670d-710e-4757-9642-02e2027c086c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question(model, tokenizer, question, context):\n",
    "    input_attention_ids = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    inputs = dict(input_attention_ids)\n",
    "\n",
    "    result = model.infer(inputs=inputs)\n",
    "\n",
    "    start_index, end_index = result.values()\n",
    "\n",
    "    # Find the start and end indices of the answer span\n",
    "    start_index = start_index.argmax()\n",
    "    end_index = end_index.argmax() + 1\n",
    "\n",
    "    # Get the answer tokens and decode them\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098e937c-d0c9-468b-8e1f-7ccf27c15fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Neural Network Exchange\n"
     ]
    }
   ],
   "source": [
    "question = \"What is ONNX? \"\n",
    "context = \"ONNX (Open Neural Network Exchange) is an open standard format for representing machine learning models.\"\n",
    "answer = answer_question(infer_request, tokenizer, question, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b210f80-3000-451c-a89a-a99293e195ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
